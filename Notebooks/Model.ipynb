{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from datetime import datetime\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "BUCKET_NAME = \"sagemaker-strokeprediction-mlops\"\n",
    "BUCKET = f's3://{BUCKET_NAME}'\n",
    "\n",
    "RAW_DATA_FOLDER = 'Dataset'\n",
    "RAW_DATA_FILE = 'healthcare-dataset-stroke-data.csv'\n",
    "RAW_DATA_PATH = os.path.join(BUCKET, RAW_DATA_FOLDER, RAW_DATA_FILE)\n",
    "\n",
    "\n",
    "TARGET_COLUMN = 'stroke'\n",
    "\n",
    "def extract_features_types(df, unique_threshold=10):\n",
    "    numerical_features = []\n",
    "    categorical_features = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col == TARGET_COLUMN:\n",
    "            continue\n",
    "        if df[col].nunique() <= unique_threshold:\n",
    "            categorical_features.append(col)\n",
    "        else:\n",
    "            numerical_features.append(col)\n",
    "\n",
    "    return numerical_features, categorical_features\n",
    "\n",
    "def split_dataset(dataset, target_column, test_size=0.2, validation_size=0.2, random_state=None):\n",
    "    X = dataset.drop(target_column, axis=1)\n",
    "    y = dataset[target_column]\n",
    "    \n",
    "    # Split dataset into train and test sets using StratifiedShuffleSplit\n",
    "    stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    for train_index, test_index in stratified_split.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Split the remaining data into validation and train sets using StratifiedShuffleSplit\n",
    "    stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=validation_size, random_state=random_state)\n",
    "    \n",
    "    for train_index, val_index in stratified_split.split(X_train, y_train):\n",
    "        X_train, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "\n",
    "def process_target(df: pd.DataFrame, col_target: str) -> pd.DataFrame:\n",
    "    \n",
    "    # Make sure that the 0 error type is also mapped to 1 (we do a binary classification later)\n",
    "    df.loc[df[col_target] == 0, col_target] = 1\n",
    "    \n",
    "    df = fill_nulls(df=df, col=col_target)\n",
    "    \n",
    "    # Reorder columns\n",
    "    colnames = list(df.columns)\n",
    "    colnames.insert(0, colnames.pop(colnames.index(col_target)))\n",
    "    df = df[colnames]\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shape of data is: (5110, 12)\n",
      "Splitting 5110 rows of data into train, validation, test datasets.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read raw input data\n",
    "df = pd.read_csv('../Dataset/healthcare-dataset-stroke-data.csv')\n",
    "logger.info(f\"Shape of data is: {df.shape}\")\n",
    "\n",
    "numerical_features, categorical_features = extract_features_types(df)\n",
    "    \n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer(n_neighbors=5)),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger.info(\"Splitting %d rows of data into train, validation, test datasets.\", len(df))\n",
    "\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = split_dataset(df, TARGET_COLUMN)\n",
    "\n",
    "# Apply preprocessor.fit_transform to train, validation and test before writing them to directories\n",
    "X_train = pd.DataFrame(preprocessor.fit_transform(X_train), index=y_train.index)\n",
    "X_val = pd.DataFrame(preprocessor.transform(X_val), index=y_val.index)\n",
    "X_test = pd.DataFrame(preprocessor.transform(X_test), index=y_test.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 388D-BB99\n",
      "\n",
      " Directory of c:\\Users\\kheri\\OneDrive - Carleton University\\dev\\Personal Coding Projects\\Stroke Prediction\\Notebooks\n",
      "\n",
      "2023-07-20  06:53 PM    <DIR>          .\n",
      "2023-07-20  06:53 PM    <DIR>          ..\n",
      "2023-07-19  11:52 AM         5,640,938 EDA_SageMaker.ipynb\n",
      "2023-07-19  12:24 PM             1,824 logs.log\n",
      "2023-07-18  07:27 PM                 0 Model.ipynb\n",
      "2023-07-20  06:53 PM            19,951 model.tar.gz\n",
      "2023-07-20  06:44 PM    <DIR>          rubish\n",
      "               4 File(s)      5,662,713 bytes\n",
      "               3 Dir(s)  160,156,835,840 bytes free\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Successfully Written Locally\n"
     ]
    }
   ],
   "source": [
    "# Create local output directories. These directories live on the container that is spun up.\n",
    "os.makedirs(\"processing/train\")\n",
    "os.makedirs(\"processing/validation\")\n",
    "os.makedirs(\"processing/test\")\n",
    "\n",
    "# Save data locally on the container that is spun up.\n",
    "try:\n",
    "    pd.concat([y_train, X_train], axis=1).to_csv(\"processing/train/train.csv\", index=False)\n",
    "    pd.concat([y_val, X_val], axis=1).to_csv(\"processing/validation/val.csv\", index=False)\n",
    "    pd.concat([y_test, X_test], axis=1).to_csv(\"processing/test/test.csv\", index=False)\n",
    "    logger.info(\"Files Successfully Written Locally\")\n",
    "except Exception as e:\n",
    "    logger.debug(\"Could Not Write the Files\")\n",
    "    logger.debug(e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:00:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:553: \n",
      "  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n",
      "  older XGBoost, please export the model by calling `Booster.save_model` from that version\n",
      "  first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = \"model.tar.gz\"\n",
    "with tarfile.open(model_path) as tar:\n",
    "    tar.extractall(path=\"..\")\n",
    "\n",
    "model = pickle.load(open(\"../xgboost-model\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing predictions against test data.\n",
      "Performing predictions against test data.\n",
      "Performing predictions against test data.\n",
      "Performing predictions against test data.\n",
      "Performing predictions against test data.\n",
      "Performing predictions against test data.\n",
      "Performing predictions against test data.\n",
      "Performing predictions against test data.\n",
      "Performing predictions against test data.\n",
      "Performing predictions against test data.\n",
      "Performing predictions against test data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:06:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:553: \n",
      "  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n",
      "  older XGBoost, please export the model by calling `Booster.save_model` from that version\n",
      "  first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.\n",
      "\n",
      "Loading test input data\n",
      "Shape of y_test: (1022,)\n",
      "Shape of dataframe after dropping labels: (1022, 23)\n",
      "Shape of predictions: (1022,)\n",
      "Creating classification evaluation report\n",
      "Classification report:\n",
      "{'binary_classification_metrics': {'accuracy': {'value': 0.952054794520548, 'standard_deviation': 'NaN'}, 'recall': {'value': 0.04, 'standard_deviation': 'NaN'}, 'precision': {'value': 0.6666666666666666, 'standard_deviation': 'NaN'}}}\n",
      "Saving classification report to evaluation\\evaluation.json\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# Author:      Kheri Hughes @ HMC - 2023\n",
    "# Description: This script contains the evaluation logic.\n",
    "# ================================================================================\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import tarfile\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    recall_score,\n",
    "    precision_score\n",
    ")\n",
    "\n",
    "model_path = \"model.tar.gz\"\n",
    "with tarfile.open(model_path) as tar:\n",
    "    tar.extractall(path=\"..\")\n",
    "\n",
    "logger.debug(\"Loading xgboost model.\")\n",
    "model = pickle.load(open(\"../xgboost-model\", \"rb\"))\n",
    "\n",
    "print(\"Loading test input data\")\n",
    "test_path = \"processing/test/test.csv\"\n",
    "df = pd.read_csv(test_path, header=0)\n",
    "\n",
    "logger.debug(\"Reading test data.\")\n",
    "y_test = df.iloc[:, 0].to_numpy()\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Debug code - Check the shapes of your loaded data\n",
    "print(f\"Shape of y_test: {y_test.shape}\")\n",
    "print(f\"Shape of dataframe after dropping labels: {df.shape}\")\n",
    "\n",
    "X_test = xgboost.DMatrix(df.values)\n",
    "\n",
    "logger.info(\"Performing predictions against test data.\")\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Debug code - Check the shapes of your predictions\n",
    "print(f\"Shape of predictions: {predictions.shape}\")\n",
    "\n",
    "# y_test = y_test.astype(int)\n",
    "# predictions = predictions.round().astype(int)\n",
    "\n",
    "print(\"Creating classification evaluation report\")\n",
    "acc = accuracy_score(y_test, predictions.round())\n",
    "recall = recall_score(y_test, predictions.round())\n",
    "precision = precision_score(y_test, predictions.round())\n",
    "\n",
    "report_dict = {\n",
    "    \"binary_classification_metrics\": {\n",
    "        \"accuracy\": {\n",
    "            \"value\": acc,\n",
    "            \"standard_deviation\": \"NaN\",\n",
    "        },\n",
    "        \"recall\": {\n",
    "            \"value\": recall,\n",
    "            \"standard_deviation\": \"NaN\"\n",
    "        },\n",
    "        \"precision\": {\n",
    "            \"value\": precision,\n",
    "            \"standard_deviation\": \"NaN\"\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Classification report:\\n{}\".format(report_dict))\n",
    "\n",
    "evaluation_dir = \"evaluation\"\n",
    "os.makedirs(evaluation_dir, exist_ok=True)\n",
    "evaluation_output_path = os.path.join(evaluation_dir, \"evaluation.json\")\n",
    "print(\"Saving classification report to {}\".format(evaluation_output_path))\n",
    "\n",
    "with open(evaluation_output_path, \"w\") as f:\n",
    "    f.write(json.dumps(report_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stroke' '0' '0' '0' '0']\n",
      "[0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_test[:5])\n",
    "print(predictions.round()[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stroke    0.000000\n",
      "0         1.349604\n",
      "1        -1.660928\n",
      "2        -0.935928\n",
      "3        -1.480927\n",
      "4         0.000000\n",
      "5         1.000000\n",
      "6         1.000000\n",
      "7         0.000000\n",
      "8         1.000000\n",
      "9         0.000000\n",
      "10        1.000000\n",
      "11        0.000000\n",
      "12        0.000000\n",
      "13        0.000000\n",
      "14        0.000000\n",
      "15        0.000000\n",
      "16        1.000000\n",
      "17        1.000000\n",
      "18        0.000000\n",
      "19        1.000000\n",
      "20        0.000000\n",
      "21        0.000000\n",
      "22        0.000000\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_path = \"processing/test/test.csv\"\n",
    "df = pd.read_csv(test_path, header=0)\n",
    "\n",
    "# choose the first row of df to test\n",
    "single_row = df.iloc[0]\n",
    "\n",
    "print(single_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker\n",
      "  Using cached sagemaker-2.173.0.tar.gz (854 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting attrs<24,>=23.1.0\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting boto3<2.0,>=1.26.131\n",
      "  Using cached boto3-1.28.8-py3-none-any.whl (135 kB)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in c:\\users\\kheri\\anaconda3\\lib\\site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in c:\\users\\kheri\\anaconda3\\lib\\site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in c:\\users\\kheri\\anaconda3\\lib\\site-packages (from sagemaker) (1.23.5)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in c:\\users\\kheri\\anaconda3\\lib\\site-packages (from sagemaker) (3.19.6)\n",
      "Collecting smdebug_rulesconfig==1.0.1\n",
      "  Using cached smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in c:\\users\\kheri\\anaconda3\\lib\\site-packages (from sagemaker) (6.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kheri\\anaconda3\\lib\\site-packages (from sagemaker) (23.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\kheri\\anaconda3\\lib\\site-packages (from sagemaker) (1.5.3)\n",
      "Collecting pathos\n",
      "  Using cached pathos-0.3.0-py3-none-any.whl (79 kB)\n",
      "Collecting schema\n",
      "  Using cached schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: PyYAML~=6.0 in c:\\users\\kheri\\anaconda3\\lib\\site-packages (from sagemaker) (6.0)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\kheri\\anaconda3\\lib\\site-packages (from sagemaker) (4.17.3)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\kheri\\anaconda3\\lib\\site-packages (from sagemaker) (2.5.2)\n",
      "Collecting tblib==1.7.0\n",
      "  Using cached tblib-1.7.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting botocore<1.32.0,>=1.31.8\n",
      "  Using cached botocore-1.31.8-py3-none-any.whl (11.0 MB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Using cached s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\kheri\\anaconda3\\lib\\site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.16.0)\n",
      "Requirement already satisfied: six in c:\\users\\kheri\\anaconda3\\lib\\site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\kheri\\anaconda3\\lib\\site-packages (from jsonschema->sagemaker) (0.18.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kheri\\anaconda3\\lib\\site-packages (from pandas->sagemaker) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\kheri\\anaconda3\\lib\\site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Collecting multiprocess>=0.70.14\n",
      "  Using cached multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "Collecting ppft>=1.7.6.6\n",
      "  Using cached ppft-1.7.6.6-py3-none-any.whl (52 kB)\n",
      "Collecting dill>=0.3.6\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "Collecting pox>=0.3.2\n",
      "  Using cached pox-0.3.2-py3-none-any.whl (29 kB)\n",
      "Collecting contextlib2>=0.5.5\n",
      "  Using cached contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\kheri\\anaconda3\\lib\\site-packages (from botocore<1.32.0,>=1.31.8->boto3<2.0,>=1.26.131->sagemaker) (1.26.16)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py): started\n",
      "  Building wheel for sagemaker (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemaker: filename=sagemaker-2.173.0-py2.py3-none-any.whl size=1163321 sha256=cf94a13112e0c1c07a968631d9f1dcbc92896e25e178891d8fc3aee72e506edc\n",
      "  Stored in directory: c:\\users\\kheri\\appdata\\local\\pip\\cache\\wheels\\67\\85\\1a\\37f9b8379d63e49fe8626a2624de3c9a74f1ed1ecc643a9c80\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: tblib, smdebug_rulesconfig, ppft, pox, jmespath, dill, contextlib2, attrs, schema, multiprocess, botocore, s3transfer, pathos, boto3, sagemaker\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 22.1.0\n",
      "    Uninstalling attrs-22.1.0:\n",
      "      Successfully uninstalled attrs-22.1.0\n",
      "Successfully installed attrs-23.1.0 boto3-1.28.8 botocore-1.31.8 contextlib2-21.6.0 dill-0.3.6 jmespath-1.0.1 multiprocess-0.70.14 pathos-0.3.0 pox-0.3.2 ppft-1.7.6.6 s3transfer-0.6.1 sagemaker-2.173.0 schema-0.7.5 smdebug_rulesconfig-1.0.1 tblib-1.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class RealTimePredictor has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Predictor.__init__() missing 1 required positional argument: 'endpoint_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m role \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39myour-sagemaker-role-arn\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[39m# Create a SageMaker predictor\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m predictor \u001b[39m=\u001b[39m sagemaker\u001b[39m.\u001b[39;49mpredictor\u001b[39m.\u001b[39;49mRealTimePredictor(\n\u001b[0;32m     10\u001b[0m     endpoint\u001b[39m=\u001b[39;49mendpoint_name,\n\u001b[0;32m     11\u001b[0m     sagemaker_session\u001b[39m=\u001b[39;49msagemaker\u001b[39m.\u001b[39;49mSession(),\n\u001b[0;32m     12\u001b[0m     content_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext/csv\u001b[39;49m\u001b[39m\"\u001b[39;49m,  \u001b[39m# The content type of the input data\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m     role\u001b[39m=\u001b[39;49mrole,\n\u001b[0;32m     14\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\kheri\\anaconda3\\lib\\site-packages\\sagemaker\\deprecations.py:252\u001b[0m, in \u001b[0;36mdeprecated_class.<locals>.DeprecatedClass.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Provides a warning for the class name.\"\"\"\u001b[39;00m\n\u001b[0;32m    251\u001b[0m renamed_warning(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe class \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 252\u001b[0m \u001b[39msuper\u001b[39m(DeprecatedClass, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: Predictor.__init__() missing 1 required positional argument: 'endpoint_name'"
     ]
    }
   ],
   "source": [
    "%pip install sagemaker\n",
    "import sagemaker\n",
    "\n",
    "# Specify your SageMaker endpoint name and role\n",
    "endpoint_name = \"your-endpoint-name\"\n",
    "role = \"your-sagemaker-role-arn\"\n",
    "\n",
    "# Create a SageMaker predictor\n",
    "predictor = sagemaker.predictor.RealTimePredictor(\n",
    "    endpoint=endpoint_name,\n",
    "    sagemaker_session=sagemaker.Session(),\n",
    "    content_type=\"text/csv\",  # The content type of the input data\n",
    "    role=role,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
